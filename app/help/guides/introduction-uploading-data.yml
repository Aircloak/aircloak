title: An introduction to uploading data
order: 1
content: |

  # Getting data into the system

  Before any analysis can take place, you need to import some data into your cloak cluster.

  Depending on your particular case, you might want to upload data all at once, or continuously as it becomes available to you.
  Either way, the data should describe discrete logical entities. An entity can be a human user, a car,
  or any other object that can be uniquely identified.
  For the remainder of this guide we are assuming the entity is a human user.

  The cloak does not impose any restrictions on how you name your users, but if you want to upload more data for a
  user who already has data in a cloak, the identifying name used, has to be the same as the one previously used.

  Before being able to upload data to a cluster, you need to prepare tables into which the data will be stored<%
    if current_user.analyst.undeleted_user_tables.count > 0 -%>
  , but that you already have done.
  <% else -%>
  .
  <% end -%>
  These tables dictate the format the cluster expects the data you upload to be in.


  ## Creating database tables

  Before uploading data to a cluster, you need to create the database tables that will hold the data in the cluster.
  It is good practise to name the table after what it stores. If you track user _location_ datapoints, you would for example
  create a table called __locations__.

  When defining a table you specify what columns the table contains. If you track entities in a cartesian coordinate space,
  you might have columns named __x__ and __y__ as well as __x_error__ and __y_error__, or if you use longitudes and
  latitudes, you could call your columns __lat__ and __lng__.

  A table can evolve over time. New columns can be added, and old ones removed. Getting the table design completely
  right from the outset is therefore not critical.

  For more information on creating tables, check out <%= help_link "managing-data" %>.


  ## Uploading data

  Once you have created database tables to hold your data, you can start uploading data into your cluster.

  The data is uploaded through an HTTP API endpoint that accepts the data encoded as JSON.
  For technical information about the HTTP API, please have a look at
  the <%= site_link "/apidocs/index.html#bulk-insert", "API Docs" %>.


  ### Data format

  #### Format

  The payload should be a JSON object with the user ids at the highest level. For each user you can pass in
  a JSON object containing an entry per table, where the table entry in turn can be an array of rows.

  #### Example

  <% if has_user_tables? -%>

  If you wanted to upload data from your __<%= sample_user_table_name %>__ table for the two users __user1__ and __user2__,
  you could upload a JSON payload with the following format

  ```json
  <%= sample_user_table_json_data %>
  ```

  <% else -%>

  Say you wanted to upload data for the users __user1__ and __user2__ to a table called __<%= sample_user_table_name %>__ which has an
  __x__ and a __y__ column, you would format the data as

  ```json
  <%= sample_user_table_json_data %>
  ```

  <% end -%>


  ### Authentication

  The API requires the uploader to be authenticated. Authentication is done through client certificates.
  <% if data_key_count == 0 -%>
  You can create these certificates for your team on the <%= site_link controller.keys_path, "keys page" %>.
  <% else -%>
  Your team already has <%= data_key_count %> keys. You can
  administer and download these from the <%= site_link controller.keys_path, "keys page" %>.
  <% end -%>

  How you use client certificates differs from language to language, but if using [curl](http://curl.haxx.se/), you would use
  the `--cert CertificateFile.pfx` command.

  ### Example

  Assuming you had data for the __<%= sample_user_table_name %>__ stored in a file in your current directory
  called __<%= sample_user_table_name %>.json__, and a key named <%= sample_data_key_name %>, you could issue the following
  [curl](http://curl.haxx.se/) command to upload data:

  ```bash
  curl -XPOST -k --cert <%= sample_data_key_name %> --data @<%= sample_user_table_name %>.json https://<%= sample_cloak_name %>/bulk_insert
  ```

  JSON, being pure text, can easily be compressed.
  The cloaks supported gzipped JSON:

  ```bash
  cat <%= sample_user_table_name %>.json | gzip -c | curl -XPOST -k --cert <%= sample_data_key_name %> --header "Content-Encoding: gzip" --header "Content-Type: application/json" https://<%= sample_cloak_name %>/bulk_insert --data-binary @-
  ```
